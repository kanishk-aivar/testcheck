import random
from time import sleep
import json
import datetime
from selenium.webdriver.common.by import By
from selenium.webdriver.chrome.options import Options
import undetected_chromedriver as uc
from bs4 import BeautifulSoup

PROXIES = []

JSON_FILE = 'ai_overview_results_selenium.json'

try:
    with open(JSON_FILE, 'r', encoding='utf-8') as f:
        results = json.load(f)
except Exception:
    results = []

def make_chrome_options(proxy):
    chrome_options = Options()
    chrome_options.add_argument("--headless=new")
    chrome_options.add_argument("--disable-gpu")
    chrome_options.add_argument("--window-size=1920,1080")
    chrome_options.add_argument("--disable-blink-features=AutomationControlled")
    chrome_options.add_argument("--disable-infobars")
    chrome_options.add_argument("--disable-extensions")
    chrome_options.add_argument(
        "--user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 12_3) "
        "AppleWebKit/537.36 (KHTML, like Gecko) Chrome/138.0.0.0 Safari/537.36"
    )
    if proxy:
        chrome_options.add_argument(f'--proxy-server=http://{proxy}')
    return chrome_options

def extract_ai_overview_html_bs(page_html):
    """
    Fallback parse using BeautifulSoup: looks for most likely AI Overview block based on marker text,
    not just attributes (robust to class/attribute changes).
    """
    soup = BeautifulSoup(page_html, "html.parser")
    # A. Look for any div containing marker phrases
    for div in soup.find_all("div"):
        t = div.text or ""
        if ("AI Overview" in t or 
            "AI-powered overview" in t or 
            "Gemini" in t or 
            "SGE" in t or 
            "Generated by" in t) and len(t) > 50:
            # Remove script/style children for clean text extract
            [s.decompose() for s in div.find_all(["script", "style"])]
            return str(div), div.get_text(separator="\n")
    return None, None

def get_overview_block(driver):
    # A. Try all common selectors FIRST
    selectors = [
        'div[data-md="311"]',
        'div[data-attrid*="ai_overview"]',
        'div[aria-label*="AI Overview"]',
        'div[data-attrid*="synth"]',
        'div[data-attrid*="sgx"]',
        'div[class^="wDYxhc"]',
    ]
    for sel in selectors:
        try:
            elems = driver.find_elements(By.CSS_SELECTOR, sel)
            for el in elems:
                outer_html = el.get_attribute("outerHTML")
                inner_txt = el.text
                if (outer_html and len(outer_html) > 100) and (
                    "AI Overview" in inner_txt or
                    "Gemini" in inner_txt or
                    "AI-powered overview" in inner_txt or
                    "SGE" in inner_txt or
                    "Generated by" in inner_txt
                ):
                    return outer_html, inner_txt
        except Exception:
            continue
    # B. Fallback: BeautifulSoup parse of full HTML
    outer_html, inner_txt = extract_ai_overview_html_bs(driver.page_source)
    return outer_html, inner_txt

def scrape_ai_overview(query):
    proxy = random.choice(PROXIES) if PROXIES else None
    chrome_options = make_chrome_options(proxy)
    driver = uc.Chrome(
        options=chrome_options,
        use_subprocess=True
    )
    result = {
        "searchQuery": query,
        "proxy": proxy,
        "extractedAt": datetime.datetime.now().isoformat(),
        "ai_html": None,
        "ai_text": None
    }
    try:
        url = f"https://www.google.com/search?q={query.replace(' ', '+')}&hl=en"
        driver.get(url)
        sleep(random.uniform(5, 9))
        # CAPTCHAs in headless mode will block; we simply skip (almost always marked by recaptcha text)
        if "recaptcha" in driver.page_source.lower() or "i'm not a robot" in driver.page_source.lower():
            print("CAPTCHA encountered! (Headless cannot solve. Skipping.)")
            result["ai_text"] = "CAPTCHA encountered"
        else:
            ai_html, ai_txt = get_overview_block(driver)
            if ai_html:
                print(f"\nAI Overview found for: {query} (proxy: {proxy})")
                print(f"Summary snippet:\n{'-'*40}\n{ai_txt[:500]}{'...' if len(ai_txt)>500 else ''}\n{'-'*40}")
                result["ai_html"] = ai_html
                result["ai_text"] = ai_txt
            else:
                print(f"NO AI Overview found for: {query} (proxy: {proxy})")
                debugfile = f"debug_{query.replace(' ', '_')}.html"
                with open(debugfile, "w", encoding='utf-8') as f:
                    f.write(driver.page_source)
                print(f"Debug HTML of page saved as {debugfile}")
    except Exception as e:
        print(f"Error: {e}")
    finally:
        driver.quit()
    return result

def main():
    while True:
        user_query = input("\nEnter a Google query (or 'exit' to stop): ").strip()
        if user_query.lower() in ('exit', 'quit', ''):
            print("Goodbye!")
            break
        record = scrape_ai_overview(user_query)
        results.append(record)
        with open(JSON_FILE, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False)
        print("Result saved. Sleeping before next request...")
        sleep(random.uniform(10, 25))

if __name__ == "__main__":
    main()
